{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Scala code runner version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL\n"}], "source": "!scala -version"}, {"cell_type": "markdown", "metadata": {}, "source": "Setup PySpark session"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "#config is correct for Scala 2.11, not 2.12\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .appName('BigQuery Storage & Spark DataFrames') \\\n    .config('spark.jars.packages', 'com.google.cloud.spark:spark-bigquery-with-dependencies_2.11:0.15.1-beta') \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"}, {"cell_type": "markdown", "metadata": {}, "source": "Access the Google Cloud Storage"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "from google.cloud import storage\n\ngcs_client = storage.Client()\nbucket = gcs_client.bucket('geitenemmer')"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 5 items\ndrwx------   - root root          0 2022-12-06 15:51 gs://geitenemmer/.ipynb_checkpoints\ndrwx------   - root root          0 2022-12-06 16:00 gs://geitenemmer/checkpoints\n-rwx------   3 root root    6675312 2022-12-06 15:54 gs://geitenemmer/e-shop clothing 2008.csv\ndrwx------   - root root          0 2022-12-06 15:51 gs://geitenemmer/google-cloud-dataproc-metainfo\ndrwx------   - root root          0 2022-12-06 15:47 gs://geitenemmer/notebooks\n"}], "source": "!hdfs dfs -ls \"gs://geitenemmer\""}, {"cell_type": "markdown", "metadata": {}, "source": "Read the data in correctly"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n |-- order: integer (nullable = true)\n |-- country: integer (nullable = true)\n |-- session ID: integer (nullable = true)\n |-- page 1 (main category): integer (nullable = true)\n |-- page 2 (clothing model): string (nullable = true)\n |-- colour: integer (nullable = true)\n |-- location: integer (nullable = true)\n |-- model photography: integer (nullable = true)\n |-- price: integer (nullable = true)\n |-- price 2: integer (nullable = true)\n |-- page: integer (nullable = true)\n\n"}], "source": "#read the data in\n\ndf1 = spark \\\n    .read \\\n    .option ( \"inferSchema\" , \"true\" ) \\\n    .option ( \"header\" , \"true\" ) \\\n    .option (\"delimiter\", \";\") \\\n    .csv ( \"gs://geitenemmer/e-shop clothing 2008.csv\" )\n\ndf1.printSchema()"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n |-- order: integer (nullable = true)\n |-- country: integer (nullable = true)\n |-- session_ID: integer (nullable = true)\n |-- page1_main_category: integer (nullable = true)\n |-- page2_clothing_model: string (nullable = true)\n |-- colour: integer (nullable = true)\n |-- location: integer (nullable = true)\n |-- model_photography: integer (nullable = true)\n |-- price: integer (nullable = true)\n |-- price2: integer (nullable = true)\n |-- page: integer (nullable = true)\n\n"}], "source": "#rename columns that would have a space in the name\ndf1 = df1.withColumnRenamed(\"session ID\",\"session_ID\")\ndf1 = df1.withColumnRenamed(\"page 1 (main category)\",\"page1_main_category\")\ndf1 = df1.withColumnRenamed(\"page 2 (clothing model)\",\"page2_clothing_model\")\ndf1 = df1.withColumnRenamed(\"model photography\",\"model_photography\")\ndf1 =df1.withColumnRenamed(\"price 2\",\"price2\")\n\ndf1.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "From here on the Spark analysis continues"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+-----+---+-----+-------+----------+-------------------+--------------------+------+--------+-----------------+-----+------+----+\n|year|month|day|order|country|session_ID|page1_main_category|page2_clothing_model|colour|location|model_photography|price|price2|page|\n+----+-----+---+-----+-------+----------+-------------------+--------------------+------+--------+-----------------+-----+------+----+\n|2008|    4|  1|    1|     29|         1|                  1|                 A13|     1|       5|                1|   28|     2|   1|\n|2008|    4|  1|    2|     29|         1|                  1|                 A16|     1|       6|                1|   33|     2|   1|\n|2008|    4|  1|    3|     29|         1|                  2|                  B4|    10|       2|                1|   52|     1|   1|\n|2008|    4|  1|    4|     29|         1|                  2|                 B17|     6|       6|                2|   38|     2|   1|\n|2008|    4|  1|    5|     29|         1|                  2|                  B8|     4|       3|                2|   52|     1|   1|\n|2008|    4|  1|    6|     29|         1|                  3|                 C56|     6|       1|                2|   57|     1|   4|\n|2008|    4|  1|    7|     29|         1|                  3|                 C57|     5|       1|                2|   33|     2|   4|\n|2008|    4|  1|    8|     29|         1|                  4|                 P67|     9|       5|                1|   38|     1|   4|\n|2008|    4|  1|    9|     29|         1|                  4|                 P82|     6|       4|                2|   48|     1|   5|\n|2008|    4|  1|    1|     29|         2|                  2|                 B31|     9|       5|                1|   57|     1|   2|\n|2008|    4|  1|    2|     29|         2|                  2|                 B21|    12|       1|                1|   67|     1|   2|\n|2008|    4|  1|    3|     29|         2|                  2|                 B24|    11|       2|                1|   57|     1|   2|\n|2008|    4|  1|    4|     29|         2|                  2|                 B27|     2|       3|                1|   57|     1|   2|\n|2008|    4|  1|    5|     29|         2|                  1|                 A10|     3|       4|                1|   38|     2|   1|\n|2008|    4|  1|    6|     29|         2|                  1|                 A10|     3|       4|                1|   38|     2|   1|\n|2008|    4|  1|    7|     29|         2|                  2|                 B27|     2|       3|                1|   57|     1|   2|\n|2008|    4|  1|    8|     29|         2|                  4|                  P1|     3|       1|                1|   38|     1|   1|\n|2008|    4|  1|    9|     29|         2|                  4|                 P34|     9|       6|                2|   48|     1|   2|\n|2008|    4|  1|   10|     29|         2|                  4|                 P33|     9|       5|                1|   43|     1|   2|\n|2008|    4|  1|    1|     21|         3|                  2|                 B17|     6|       6|                2|   38|     2|   1|\n+----+-----+---+-----+-------+----------+-------------------+--------------------+------+--------+-----------------+-----+------+----+\nonly showing top 20 rows\n\n"}], "source": "df1.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Defining the batches"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import *"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "df_month_4 = df1.where(df1.month == \"4\")\ndf_month_5 = df1.where(df1.month == \"5\")\ndf_month_6 = df1.where(df1.month == \"6\")\ndf_month_7 = df1.where(df1.month == \"7\")\ndf_month_8 = df1.where(df1.month == \"8\")"}, {"cell_type": "markdown", "metadata": {}, "source": "Calculating the sales days for each batch/month"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "max_day_4 = df_month_4.groupBy('day', 'month')\\\n    .agg(sum(\"price\").alias(\"sum_price\")) \\\n    .sort(desc(\"sum_price\")) \\\n\nmax_day_5 = df_month_5.groupBy('day', 'month')\\\n    .agg(sum(\"price\").alias(\"sum_price\")) \\\n    .sort(desc(\"sum_price\")) \\\n\nmax_day_6 = df_month_6.groupBy('day', 'month')\\\n    .agg(sum(\"price\").alias(\"sum_price\")) \\\n    .sort(desc(\"sum_price\")) \\\n\nmax_day_7 = df_month_7.groupBy('day', 'month')\\\n    .agg(sum(\"price\").alias(\"sum_price\")) \\\n    .sort(desc(\"sum_price\")) \\\n\nmax_day_8 = df_month_8.groupBy('day', 'month')\\\n    .agg(sum(\"price\").alias(\"sum_price\")) \\\n    .sort(desc(\"sum_price\")) \\\n"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----+---------+\n|day|month|sum_price|\n+---+-----+---------+\n|  2|    4|   150387|\n|  1|    4|   139570|\n|  3|    4|    81466|\n| 10|    4|    80551|\n|  7|    4|    80067|\n|  4|    4|    78843|\n| 18|    4|    78416|\n| 14|    4|    77543|\n| 29|    4|    77363|\n|  8|    4|    74633|\n| 16|    4|    74126|\n| 17|    4|    71210|\n| 11|    4|    70897|\n| 21|    4|    69749|\n|  5|    4|    66579|\n| 19|    4|    65131|\n|  9|    4|    64664|\n| 23|    4|    64319|\n| 22|    4|    64016|\n| 28|    4|    63914|\n| 15|    4|    61424|\n|  6|    4|    57531|\n| 12|    4|    56661|\n| 20|    4|    55043|\n| 24|    4|    53152|\n| 30|    4|    52512|\n| 25|    4|    47068|\n| 27|    4|    45079|\n| 13|    4|    42061|\n| 26|    4|    36949|\n| 14|    5|    69781|\n| 29|    5|    66287|\n| 16|    5|    65744|\n| 19|    5|    65337|\n|  7|    5|    64681|\n| 21|    5|    63043|\n| 25|    5|    62811|\n|  5|    5|    62577|\n| 12|    5|    60852|\n|  8|    5|    60416|\n| 20|    5|    58769|\n| 28|    5|    55948|\n| 27|    5|    54782|\n|  6|    5|    53690|\n| 23|    5|    50728|\n|  1|    5|    49884|\n| 15|    5|    48527|\n| 11|    5|    46513|\n| 26|    5|    46358|\n| 24|    5|    45833|\n| 13|    5|    44663|\n| 22|    5|    44239|\n|  9|    5|    43585|\n|  2|    5|    42056|\n|  4|    5|    40178|\n| 30|    5|    37176|\n| 18|    5|    37091|\n| 10|    5|    34906|\n| 17|    5|    33084|\n| 31|    5|    27017|\n|  3|    5|    26017|\n|  3|    6|    59904|\n| 10|    6|    58964|\n|  4|    6|    58594|\n| 26|    6|    57422|\n| 24|    6|    55958|\n|  9|    6|    55918|\n| 17|    6|    55624|\n| 19|    6|    54145|\n|  5|    6|    53687|\n| 18|    6|    53061|\n| 12|    6|    52464|\n| 13|    6|    52378|\n| 11|    6|    51671|\n| 16|    6|    51197|\n| 30|    6|    51153|\n| 25|    6|    49490|\n| 27|    6|    48208|\n|  2|    6|    46950|\n|  6|    6|    46889|\n| 23|    6|    46429|\n| 20|    6|    42686|\n| 29|    6|    41998|\n| 22|    6|    37475|\n|  7|    6|    36176|\n|  8|    6|    35008|\n|  1|    6|    34894|\n| 15|    6|    34174|\n| 28|    6|    33584|\n| 21|    6|    27994|\n| 14|    6|    27886|\n| 24|    7|    67617|\n| 22|    7|    66957|\n|  8|    7|    66337|\n| 17|    7|    62754|\n| 23|    7|    62658|\n| 16|    7|    61873|\n| 21|    7|    61408|\n| 14|    7|    59094|\n| 15|    7|    56084|\n+---+-----+---------+\nonly showing top 100 rows\n\n"}], "source": "res = max_day_4.union(max_day_5)\nresult = res.union(max_day_6)\nresult_t = result.union(max_day_7)\nresult_table = result_t.union(max_day_8)\nresult_table.show(100)"}, {"cell_type": "markdown", "metadata": {}, "source": "Continue only if the table schema is setup correctly in BigQuery"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "#Define the GCS bucket\ngcs_bucket = \"geitenemmer\"\n\n#table schema needs to be the same in BigQuery\n\n#write to an existing dataset.table\nresult_table.write \\\n    .format(\"bigquery\") \\\n    .option(\"temporaryGcsBucket\", gcs_bucket) \\\n    .option(\"checkpointLocation\", \"gs://geitenemmer/checkpoints\") \\\n    .option(\"table\", \"batch_dataset.result_table\") \\\n    .mode(\"append\") \\\n    .save()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "%%bigquery\nSELECT month\nFROM batch_dataset.result_table\nORDER BY month DESC\nLIMIT 10"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 2}